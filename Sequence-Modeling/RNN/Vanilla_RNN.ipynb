{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e566f08-e7c7-46b4-b93f-4d6159dd40ee",
   "metadata": {},
   "source": [
    "## Implementing a Vanilla RNN with Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50a80a50-81bf-4454-9f58-7d8ed815631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa590a56-d04a-4c4d-9020-38bf4afefadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampla data \n",
    "inputs = np.array([\n",
    "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "    [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
    "    [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
    "    [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
    "    [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
    "])\n",
    "\n",
    "expected = np.array([\n",
    "    [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
    "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "    [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"], \n",
    "    [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
    "    [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2765aebe-9870-4f5e-b452-83a16122bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_one_hot(inputs: np.ndarray) -> np.ndarray:\n",
    "    char_to_index = {char: i for i, char in enumerate(string.ascii_uppercase)}\n",
    "    # string.ascii_uppercase is a string containing all uppercase letters \n",
    "    # enumerate(string.ascii_uppercase) pairs each charatacter with its index \n",
    "    # {char: i for i, char in enume...} creates the mapping\n",
    "\n",
    "    one_hot_inputs = [] # empty list of one_hot_inputs\n",
    "    for row in inputs: # starts loop over each row in the input \n",
    "        one_hot_list = []\n",
    "        for char in row:\n",
    "            if char.upper() in char_to_index:\n",
    "                one_hot_vector = np.zeros((len(string.ascii_uppercase), 1))\n",
    "                one_hot_vector[char_to_index[char.upper()]] = 1\n",
    "                one_hot_list.append(one_hot_vector)\n",
    "        one_hot_inputs.append(one_hot_list)\n",
    "\n",
    "    return np.array(one_hot_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52a07ab6-bc21-4384-8b6f-e67e8895fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Layer \n",
    "class InputLayer: \n",
    "    inputs: np.ndarray \n",
    "    U: np.ndarray = None \n",
    "    delta_U: np.ndarray = None \n",
    "\n",
    "    def __init__(self, inputs: np.ndarray, hidden_size: int) -> None: \n",
    "        self.inputs = inputs \n",
    "        self.U = np.random.uniform(low = 0, high = 1, size = (hidden_size, len(inputs[0])))\n",
    "        self.delta_U = np.zeros_like(self.U)\n",
    "    # get_input -> Return the one-hot encoded vector of character at a given step \n",
    "    def get_input(self, time_step: int) -> np.ndarray:\n",
    "        return self.U @ self.get_input(time_step)\n",
    "    # return the result of U . x[t] to be used in s_t calculation\n",
    "    def weighted_sum(self, time_step: int) -> np.ndarray: \n",
    "        return self.U @ self.get_input(time_step)\n",
    "\n",
    "    def calculate_deltas_per_step(\n",
    "        self, time_step: int, delta_weighted_sum: np.ndarray\n",
    "    ) -> None: \n",
    "        self.delta_U += delta_weighted_sum @ self.get_input(time_step).T \n",
    "    # Returns the parameters using the gradient U = U - \\alpha*dL/dU\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None: \n",
    "        self.U -= learning_rate * self.delta_U "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbd31e19-e2e6-4c46-9938-d8cd70ac9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layer \n",
    "class HiddenLayer: \n",
    "    states: np.ndarray = None # Stores activation of all time steps (internal memory of the network)\n",
    "    W: np.ndarray = None # Recurrent weight matrix \n",
    "    delta_W: np.ndarray = None # gradient W of diuring BPTT \n",
    "    bias: np.ndarray = None # bias in math formulas \n",
    "    delta_bias: np.ndarray = None  # gradient of b \n",
    "    next_delta_activation: np.ndarray = None # derivative of next step's loss function w.r.t current activation, from\n",
    "    # this formula \n",
    "\n",
    "    def __init__(self, vocab_size: int, size: int) -> None: \n",
    "        self.W = np.random.uniform(low = 0, high = 1, size = (size, size))\n",
    "        self.bias = np.random.uniform(low = 0, high = 1, size = (size, 1))\n",
    "        self.states = np.zeros(shape=(vocab_size, size, 1))\n",
    "        self.next_delta_activation = np.zeros(shape=(size, 1))\n",
    "        self.delta_bias = np.zeros_like(self.bias)\n",
    "        self.delta_W = np.zeros_like(self.W)\n",
    "    # Return the hidden state value at a given step. if time step is less than 0, default to all zeros matrix \n",
    "    def get_hidden_state(self, time_step: int) -> np.ndarray: \n",
    "        # If starting out at the beginning of the sequence, a[t - 1] will return zeros \n",
    "        if time_step < 0: \n",
    "            return np.zeros_like(self.states[0])\n",
    "        return self.states[time_step]\n",
    "    # Updating the state at a time steep after forward pass calculation\n",
    "    def set_hidden_state(self, time_step: int, hidden_state: np.ndarray) -> None: \n",
    "        self.states[time_step] = hidden_state \n",
    "    # Forward pass calculation \n",
    "    def activate(self, weighted_input: np.ndarray, time_step: int) -> np.ndarray: \n",
    "        previous_hidden_state = self.get_hidden_state(time_step - 1)\n",
    "        # W @ h_prev => (h_dimension, h_dimension) @ (h_dimension, 1) = (h_dimension, 1)\n",
    "        weighted_hidden_state = self.W @ previous_hidden_state \n",
    "        # (h_dimension, 1) + (h_dimension, 1) + (h_dimension, 1) = (h_dimension, 1)\n",
    "        weighted_sum = weighted_input + weighted_hidden_stae + self.bias \n",
    "        activation = np.tanh(weighted_sum) # (h_dimension, 1)\n",
    "        self.hidden_state(time_step, activation)\n",
    "        return activation\n",
    "\n",
    "    def calculate_deltas_per_step(\n",
    "        self, time_step: int, delta_output: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        # (h_dimension, 1) + (h_dimension, 1) = (h_dimension, 1)\n",
    "        delta_activatiation = delta_output + self.next_delta_activation\n",
    "        # (h_dimension, 1) * scalar = (h_dimension, 1) \n",
    "        delta_weighted_sum = delta_activation * (\n",
    "            1 - self.get_hidden_state(time_step) ** 2 \n",
    "        )\n",
    "        # (h_dimension, h_dimension) @ (h_dimension, 1) = (h_dimension, 1)\n",
    "        self.next_delta_activation = self.W.T @ delta_weighed_sum \n",
    "\n",
    "        # (h_dimension, 1) @ (1, h_dimension) = (h_dimension, h_dimension)\n",
    "        self.delta_W += delta_weighed_sum @ self.get_hidden_state(time_step - 1).T \n",
    "\n",
    "        # Derivative of hidden bias is the same as dL_ds \n",
    "        self.delta_bias += delta_weighed_sum\n",
    "        return delta_weighed_sum\n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None: \n",
    "        self.W -= learning_rate * self.delta_W \n",
    "        self.bias -= learning_rate * self.delta_bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a32b5748-cdcf-4829-9115-d6d5c73f3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Layer \n",
    "class OutputLayer:\n",
    "    states: np.ndarray = None # Stores predictions of all time  steps \n",
    "    V: np.ndarray = None # Output weights matrix \n",
    "    bias: np.ndarray = None # c in math formulas\n",
    "    delta_bias: np.ndarray = None # gradient of c\n",
    "    delta_V: np.ndarray = None # output of weight matrix\n",
    "\n",
    "    def __init__(self, size: int, hidden_size: int) -> None: \n",
    "        self.V = np.random.uniform(low = 0, high = 1, size=(size, hidden_size))\n",
    "        self.bias = np.random.uniform(low=0, high=1, size=(size, 1))\n",
    "        self.delta_bias = np.zeros_like(self.bias)\n",
    "        self.delta_V = np.zeros_like(self.V)\n",
    "    # predict: forward pass to calculate the weighted output and probability distribution with softmax \n",
    "    def predict(self, hidden_state: np.ndarray, time_step: int) -> np.ndarray: \n",
    "        # V @ h => (input_size, h_dimension) @ (h_dimension, 1) = (input_size, 1)\n",
    "        # (input_size, 1) + (input_size, 1) = (input_size, 1)\n",
    "        output = self.V @ hidden_state + self.bias \n",
    "        prediction = sofmax(output)\n",
    "        self.set_state(time_step, prediction)\n",
    "        return prediction \n",
    "    # Return the output state (prediction) value at a given time step\n",
    "    def get_state(self, time_step: int) -> np.ndarray: \n",
    "        return self.states[time_step]\n",
    "    # Updating the output state at a time step after forward pass calculation\n",
    "    def set_state(self, time_step: int, prediction: np.ndarray) -> None: \n",
    "        self.states[time_step] = prediction\n",
    "    # compute gradients of v and c\n",
    "    def calculate_deltas_per_step(\n",
    "        self, \n",
    "        expected: np.ndarray, \n",
    "        hidden_state: np.ndarray, \n",
    "        time_step: int, \n",
    "    ) -> np.ndarray:\n",
    "        # dL_do = dL_dyhat * dyhat_do = derivative of loss function * derivative of softmax \n",
    "        # dl_do = step.y_hat - expected[step_number]\n",
    "        delta_output = self.get_state(time_step) - expected # (input_size, 1)\n",
    "\n",
    "        # (input_size, 1) @ (1, hidden_size) = (input_size, hidden_size)\n",
    "        self.delta_V += delta_output @ hidden_state.T \n",
    "\n",
    "        # dL_dc += dL_do \n",
    "        self.delta_bias += delta_output \n",
    "        return self.V.T @ delta_output \n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None: \n",
    "        self.V -= learning_rate * self.delta_V\n",
    "        self.bias -= learning_rate * self.delta_bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b81936d7-41f1-42bb-afbe-2e9db7a36ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    hidden_layer: HiddenLayer\n",
    "    output_layer: OutputLayer\n",
    "    alpha: float  # learning rate\n",
    "    input_layer: InputLayer = None\n",
    "\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, alpha: float) -> None:\n",
    "        self.hidden_layer = HiddenLayer(vocab_size, hidden_size)\n",
    "        self.output_layer = OutputLayer(vocab_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def feed_forward(self, inputs: np.ndarray) -> OutputLayer:\n",
    "        self.input_layer = InputLayer(inputs, self.hidden_size)\n",
    "        for step in range(len(inputs)):\n",
    "            weighted_input = self.input_layer.weighted_sum(step)\n",
    "            activation = self.hidden_layer.activate(weighted_input, step)\n",
    "            self.output_layer.predict(activation, step)\n",
    "        return self.output_layer\n",
    "\n",
    "    def backpropagation(self, expected: np.ndarray) -> None:\n",
    "        for step_number in reversed(range(len(expected))):\n",
    "            delta_output = self.output_layer.calculate_deltas_per_step(\n",
    "                expected[step_number],\n",
    "                self.hidden_layer.get_hidden_state(step_number),\n",
    "                step_number,\n",
    "            )\n",
    "            delta_weighted_sum = self.hidden_layer.calculate_deltas_per_step(\n",
    "                step_number, delta_output\n",
    "            )\n",
    "            self.input_layer.calculate_deltas_per_step(step_number, delta_weighted_sum)\n",
    "\n",
    "        self.output_layer.update_weights_and_bias(self.alpha)\n",
    "        self.hidden_layer.update_weights_and_bias(self.alpha)\n",
    "        self.input_layer.update_weights_and_bias(self.alpha)\n",
    "\n",
    "    def loss(self, y_hat: list[np.ndarray], y: list[np.ndarray]) -> float:\n",
    "        \"\"\"\n",
    "        Cross-entropy loss function - Calculating difference between 2 probability distributions.\n",
    "        First, calculate cross-entropy loss for each time step with np.sum, which returns a numpy array\n",
    "        Then, sum across individual losses of all time steps with sum() to get a scalar value.\n",
    "        :param y_hat: predicted value\n",
    "        :param y: expected value - true label\n",
    "        :return: total loss\n",
    "        \"\"\"\n",
    "        return sum(-np.sum(y[i] * np.log(y_hat[i]) for i in range(len(y))))\n",
    "\n",
    "    def train(self, inputs: np.ndarray, expected: np.ndarray, epochs: int) -> None:\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"epoch={epoch}\")\n",
    "            for idx, input in enumerate(inputs):\n",
    "                y_hats = self.feed_forward(input)\n",
    "                self.backpropagation(expected[idx])\n",
    "                print(\n",
    "                    f\"Loss round: {self.loss([y for y in y_hats.states], expected[idx])}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52aa81a7-9116-4f93-9e4d-74652f4f3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_inputs = string_to_one_hot(inputs)\n",
    "one_hot_expected = string_to_one_hot(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f58956e-d1b6-43b4-a835-ebdeb237886f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rnn \u001b[38;5;241m=\u001b[39m VanillaRNN(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(string\u001b[38;5;241m.\u001b[39mascii_uppercase), hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_hot_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_expected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 52\u001b[0m, in \u001b[0;36mVanillaRNN.train\u001b[0;34m(self, inputs, expected, epochs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inputs):\n\u001b[0;32m---> 52\u001b[0m     y_hats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackpropagation(expected[idx])\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss round: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss([y\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39my\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39my_hats\u001b[38;5;241m.\u001b[39mstates],\u001b[38;5;250m \u001b[39mexpected[idx])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m, in \u001b[0;36mVanillaRNN.feed_forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer \u001b[38;5;241m=\u001b[39m InputLayer(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)):\n\u001b[0;32m---> 16\u001b[0m     weighted_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweighted_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer\u001b[38;5;241m.\u001b[39mactivate(weighted_input, step)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer\u001b[38;5;241m.\u001b[39mpredict(activation, step)\n",
      "Cell \u001b[0;32mIn[30], line 16\u001b[0m, in \u001b[0;36mInputLayer.weighted_sum\u001b[0;34m(self, time_step)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweighted_sum\u001b[39m(\u001b[38;5;28mself\u001b[39m, time_step: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray: \n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU \u001b[38;5;241m@\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m, in \u001b[0;36mInputLayer.get_input\u001b[0;34m(self, time_step)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, time_step: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU \u001b[38;5;241m@\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m, in \u001b[0;36mInputLayer.get_input\u001b[0;34m(self, time_step)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, time_step: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU \u001b[38;5;241m@\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: InputLayer.get_input at line 13 (2966 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m, in \u001b[0;36mInputLayer.get_input\u001b[0;34m(self, time_step)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, time_step: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU \u001b[38;5;241m@\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "rnn = VanillaRNN(vocab_size=len(string.ascii_uppercase), hidden_size=128, alpha=0.0001)\n",
    "rnn.train(one_hot_inputs, one_hot_expected, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a377eef-898e-4d6b-a354-7597200cd4fc",
   "metadata": {},
   "source": [
    "## Vanilla RNN in PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02d0b2b6-a59d-4703-a73a-f19ac0956056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.2564\n",
      "Epoch [2/10], Loss: 3.2435\n",
      "Epoch [3/10], Loss: 3.2306\n",
      "Epoch [4/10], Loss: 3.2176\n",
      "Epoch [5/10], Loss: 3.2045\n",
      "Epoch [6/10], Loss: 3.1911\n",
      "Epoch [7/10], Loss: 3.1773\n",
      "Epoch [8/10], Loss: 3.1630\n",
      "Epoch [9/10], Loss: 3.1482\n",
      "Epoch [10/10], Loss: 3.1327\n",
      "Predicted index: 24\n",
      "Predicted character: Y\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Prepare input and expected data\n",
    "inputs = np.array([\n",
    "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "    [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
    "    [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
    "    [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
    "    [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
    "])\n",
    "\n",
    "expected = np.array([\n",
    "    [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
    "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "    [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"],\n",
    "    [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
    "    [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "])\n",
    "\n",
    "# Encode strings to one-hot vectors\n",
    "def encode_sequences(sequences):\n",
    "    encoded = np.vectorize(string.ascii_uppercase.index)(sequences)\n",
    "    one_hot = np.eye(26)[encoded]\n",
    "    return torch.FloatTensor(one_hot)\n",
    "\n",
    "# Prepare input and target data\n",
    "X = encode_sequences(inputs)\n",
    "y = encode_sequences(expected)\n",
    "\n",
    "# RNN Model\n",
    "class AlphabetRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(AlphabetRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 26  # One-hot encoded input size\n",
    "hidden_size = 128\n",
    "output_size = 26\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = AlphabetRNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    \n",
    "    # Reshape outputs and targets for loss calculation\n",
    "    loss = criterion(outputs.view(-1, output_size), y.argmax(dim=2).view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Prediction\n",
    "new_input = np.array([[\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"]])\n",
    "new_input_encoded = encode_sequences(new_input)\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    prediction = model(new_input_encoded)\n",
    "    predicted_index = prediction[0, -1].argmax().item()\n",
    "\n",
    "print(\"Predicted index:\", predicted_index)\n",
    "print(\"Predicted character:\", string.ascii_uppercase[predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b8821-36d7-4c10-8ab5-9fc89b620982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
